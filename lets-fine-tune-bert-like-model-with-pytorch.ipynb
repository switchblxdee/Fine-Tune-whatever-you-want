{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10813981,"sourceType":"datasetVersion","datasetId":6713514}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.optim import AdamW\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import DebertaV2ForSequenceClassification, DebertaV2Tokenizer\nfrom transformers import get_linear_schedule_with_warmup\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, classification_report\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df = pd.read_csv(\"/kaggle/input/dataset-for-sub/train.csv\")\ntrain_df = train_df[[\"id\", \"text\", \"target\"]]\ntrain_df[\"text\"] = train_df[\"text\"].apply(lambda x: x.lower())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = DebertaV2ForSequenceClassification.from_pretrained(\"MoritzLaurer/deberta-v3-large-zeroshot-v1.1-all-33\", num_labels=2, ignore_mismatched_sizes=True).to(device)\ntokenizer = DebertaV2Tokenizer.from_pretrained(\"MoritzLaurer/deberta-v3-large-zeroshot-v1.1-all-33\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"texts = train_df[\"text\"].values\nlabels = train_df[\"target\"].values","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(texts, labels, test_size=.2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TextDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n    \n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        label = self.labels[idx]\n\n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding=\"max_length\",\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors=\"pt\"\n        )\n\n        return {\n            \"input_ids\": encoding[\"input_ids\"].flatten(),\n            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n            \"label\": torch.tensor(label, dtype=torch.long)\n        }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = TextDataset(X_train, y_train, tokenizer, 99)\nval_dataset = TextDataset(X_val, y_val, tokenizer, 99)\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=8,\n    shuffle=True\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=8,\n    shuffle=True\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(), lr=2e-5)\nloss_fn = nn.CrossEntropyLoss()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.train()\n\nfor epoch in range(5):\n    losses = []\n    print(f\"Epoch: {epoch + 1}/5\")\n    print(\"-\" * 100)\n\n    for batch in tqdm(train_loader):\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"label\"].to(device)\n\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n        logits = outputs.logits\n        \n        loss = loss_fn(logits, labels)\n        losses.append(loss.item())\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n    epoch_loss = sum(losses) / len(losses)\n    print(f\"Epoch {epoch+1} average loss: {epoch_loss}\")\n    print(\"-\"*100)\n    print(\"\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()\nval_losses = []\nval_preds = []\nval_true = []\n\nwith torch.no_grad():\n    for batch in tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}\"):\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n        labels = batch[\"label\"].to(device)\n\n        outputs = model(input_ids, attention_mask=attention_mask)\n        logits = outputs.logits\n\n        loss = loss_fn(logits, labels)\n        val_losses.append(loss.item())\n\n        preds = torch.argmax(logits, dim=1)\n        val_preds.extend(preds.cpu().numpy())\n        val_true.extend(labels.cpu().numpy())\n\ntrain_loss = np.mean(losses)\nval_loss = np.mean(val_losses)\nval_accuracy = accuracy_score(val_true, val_preds)\n\nprint(f\"\\nEpoch {epoch+1}\")\nprint(f\"Train Loss: {train_loss:.4f}\")\nprint(f\"Val Loss: {val_loss:.4f}\")\nprint(f\"Val Accuracy: {val_accuracy:.4f}\")\nprint(classification_report(val_true, val_preds))\nprint(\"-\" * 50)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/dataset-for-sub/test.csv\")\ntest_df = test_df[[\"id\", \"text\"]]\ntest_df[\"text\"] = test_df[\"text\"].apply(lambda x: x.lower())\ntest_df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_texts = test_df['text'].values","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class TestDataset(Dataset):\n    def __init__(self, texts, tokenizer, max_len):\n        self.texts = texts\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        \n        encoding = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt',\n        )\n        \n        return {\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten()\n        }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_dataset = TestDataset(test_texts, tokenizer, 99)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_loader = DataLoader(\n    test_dataset,\n    batch_size=32,\n    shuffle=False\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = model.eval()\npredictions = []\nprediction_probs = []\n\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc=\"Predicting\"):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        \n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask\n        )\n        \n        _, preds = torch.max(outputs.logits, dim=1)\n        \n        predictions.extend(preds.cpu().numpy())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df[\"target\"] = predictions\ntest_df = test_df[[\"id\", \"target\"]]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df.to_csv(\"sub.csv\", index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}